{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment to convert images to ELA\n",
    "\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# from pylab import *\n",
    "# import re\n",
    "# from PIL import Image, ImageChops, ImageEnhance\n",
    "\n",
    "# def convert_to_ela_image(path, quality):\n",
    "#     filename = path\n",
    "#     resaved_filename = filename.split('.')[0] + '.resaved.jpg'\n",
    "#     ELA_filename = filename.split('.')[0] + '.ela.png'\n",
    "    \n",
    "#     im = Image.open(filename).convert('RGB')\n",
    "#     im.save(resaved_filename, 'JPEG', quality=quality)\n",
    "#     resaved_im = Image.open(resaved_filename)\n",
    "    \n",
    "#     ela_im = ImageChops.difference(im, resaved_im)\n",
    "    \n",
    "#     extrema = ela_im.getextrema()\n",
    "#     max_diff = max([ex[1] for ex in extrema])\n",
    "#     if max_diff == 0:\n",
    "#         max_diff = 1\n",
    "#     scale = 255.0 / max_diff\n",
    "    \n",
    "#     ela_im = ImageEnhance.Brightness(ela_im).enhance(scale)\n",
    "    \n",
    "#     os.remove(resaved_filename)\n",
    "    \n",
    "#     return ela_im\n",
    "\n",
    "# for directory in (r\"data\\satire\\satire_images_rgb\\satire_images\", r\"data\\satire\\satire_images_rgb\\serious_images\"):\n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.endswith(\".jpg\"):\n",
    "#             file_path = os.path.join(directory, filename)\n",
    "#             pil_image = convert_to_ela_image(file_path, 90)\n",
    "#             pil_image.save(\"data\\satire\\satire_images_ela\\\" + directory.split(\"\\\\\")[-1] + \"\\\\\" + filename.split(\".\")[0] + \".png\", format=\"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14714 images belonging to 2 classes.\n",
      "Found 3678 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.2, rescale = 1./255, horizontal_flip = True)\n",
    "\n",
    "train_generator = generator.flow_from_directory(r\"data\\satire\\satire_images_ela\",\n",
    "                                      target_size = (128,128),\n",
    "                                      class_mode = 'categorical',\n",
    "                                      interpolation = 'nearest',\n",
    "                                      batch_size = batch_size,\n",
    "                                      subset = \"training\")\n",
    "\n",
    "val_generator = generator.flow_from_directory(r\"data\\satire\\satire_images_ela\",\n",
    "                                      target_size = (128,128),\n",
    "                                      class_mode = 'categorical',\n",
    "                                      interpolation = 'nearest',\n",
    "                                      batch_size = batch_size,\n",
    "                                      subset = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SATIRE_DETECTOR(input_shape=(128,128,3)):\n",
    "    X_input = Input(input_shape)\n",
    "    ##########################################################\n",
    "    X = Conv2D(32, (5,5), padding=\"valid\", activation=tf.nn.leaky_relu, name=\"conv1\")(X_input)\n",
    "    X = MaxPooling2D(pool_size=(2,2), strides=(2,2), name=\"max_pool1\")(X)\n",
    "    \n",
    "    X = Conv2D(32, (5,5), padding=\"valid\", activation=tf.nn.leaky_relu, name=\"conv2\")(X)\n",
    "    X = MaxPooling2D(pool_size=(2,2), strides=(2,2), name=\"max_pool2\")(X)\n",
    "    X = Dropout(0.25, name=\"dropout1\")(X)\n",
    "    ###########################################################\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(256, activation=tf.nn.leaky_relu, name=\"fc1\", kernel_regularizer=l2(0.0005), bias_regularizer=l2(0.0005))(X)\n",
    "    X = Dropout(0.5, name=\"dropout2\")(X)\n",
    "    \n",
    "    X = Dense(2, activation=\"softmax\", name=\"fc2\")(X)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=X_input, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SATIRE_DETECTOR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 124, 124, 32)      2432      \n",
      "_________________________________________________________________\n",
      "max_pool1 (MaxPooling2D)     (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 58, 58, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pool2 (MaxPooling2D)     (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout1 (Dropout)           (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 26912)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 256)               6889728   \n",
      "_________________________________________________________________\n",
      "dropout2 (Dropout)           (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 6,918,306\n",
      "Trainable params: 6,918,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9, decay=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "459/459 [==============================] - 356s 776ms/step - loss: 0.4224 - accuracy: 0.8936 - val_loss: 0.2810 - val_accuracy: 0.9668\n",
      "Epoch 2/10\n",
      "459/459 [==============================] - 295s 643ms/step - loss: 0.2549 - accuracy: 0.9700 - val_loss: 0.2241 - val_accuracy: 0.9833\n",
      "Epoch 3/10\n",
      "459/459 [==============================] - 264s 575ms/step - loss: 0.1993 - accuracy: 0.9848 - val_loss: 0.2167 - val_accuracy: 0.9877\n",
      "Epoch 4/10\n",
      "459/459 [==============================] - 250s 546ms/step - loss: 0.1830 - accuracy: 0.9878 - val_loss: 0.1982 - val_accuracy: 0.9863\n",
      "Epoch 5/10\n",
      "459/459 [==============================] - 250s 545ms/step - loss: 0.1769 - accuracy: 0.9888 - val_loss: 0.2148 - val_accuracy: 0.9912\n",
      "Epoch 6/10\n",
      "459/459 [==============================] - 288s 627ms/step - loss: 0.2110 - accuracy: 0.9880 - val_loss: 0.1773 - val_accuracy: 0.9877\n",
      "Epoch 7/10\n",
      "459/459 [==============================] - 382s 832ms/step - loss: 0.1532 - accuracy: 0.9913 - val_loss: 0.1953 - val_accuracy: 0.9866\n",
      "Epoch 8/10\n",
      "459/459 [==============================] - 359s 783ms/step - loss: 0.1974 - accuracy: 0.9890 - val_loss: 0.2198 - val_accuracy: 0.9896\n",
      "Epoch 9/10\n",
      "459/459 [==============================] - 342s 745ms/step - loss: 0.1601 - accuracy: 0.9917 - val_loss: 0.1547 - val_accuracy: 0.9907\n",
      "Epoch 10/10\n",
      "459/459 [==============================] - 343s 747ms/step - loss: 0.1786 - accuracy: 0.9910 - val_loss: 0.2043 - val_accuracy: 0.9888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22fb7c19080>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=val_generator,\n",
    "          steps_per_epoch=14714//batch_size,\n",
    "          validation_steps=3678//batch_size,\n",
    "          workers=1,\n",
    "          use_multiprocessing=False\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
